<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <style>
    body { font-family: sans-serif; padding: 16px; }
    button { padding: 8px 16px; margin-right: 8px; }
    textarea { width: 100%; height: 200px; margin-top: 12px; white-space: pre-wrap; }
    /* æ–°å¢æ ·å¼ */
    .status-panel {
      background: #f0f0f0;
      padding: 10px;
      border-radius: 5px;
      margin: 10px 0;
      font-size: 14px;
    }
    .status-line {
      margin: 5px 0;
    }
  </style>
</head>
<body>
  // åœ¨çŠ¶æ€é¢æ¿å‰æ·»åŠ é…ç½®ä¿¡æ¯æ˜¾ç¤º
<div class="config-info" style="background: #d4edda; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
  <strong>Recording Configuration:</strong><br>
  Processing: Every 5 minutes<br>
  Auto-summarize: Every <span id="summaryMinutes"></span> minutes (after <span id="segmentsNeeded"></span> segments)
</div>

// åœ¨scriptå¼€å§‹å¤„æ·»åŠ ï¼š
document.getElementById('intervalMin').textContent = intervalMin;
document.getElementById('segmentsNeeded').textContent = SUMMARIZE_FREQUENCY;
  <h2>ğŸ™ï¸ Record Meeting</h2>
  
  <!-- æ–°å¢çŠ¶æ€é¢æ¿ -->
  <div class="status-panel">
    <div class="status-line" id="recordingStatus"></div>
    <div class="status-line" id="segmentStatus"></div>
  </div>
  
  <button id="startBtn">Start</button>
  <button id="stopBtn" disabled>Stop</button>
  
  <textarea id="textArea" placeholder="Transcript and summary will appear here..."></textarea>

  <script>
    // ========== é…ç½®å¸¸é‡ - ä¿®æ”¹ä¸º3åˆ†é’Ÿ ==========
    const SEGMENT_DURATION = 5 * 60 * 1000;  // æ”¹ä¸º3åˆ†é’Ÿ (åŸæœ¬æ˜¯30ç§’)
    const intervalMin = parseInt(urlParams.get("intervalMin") || "10");  // é»˜è®¤10åˆ†é’Ÿ
    const SUMMARIZE_FREQUENCY = Math.ceil(intervalMin / 5);  // è®¡ç®—éœ€è¦å‡ ä¸ªsegments
   
    
    // ========== ä¿ç•™åŸæœ‰çš„å˜é‡ ==========
    let contextPdfText = "";
    let sessionStartTime = null;
    const urlParams = new URLSearchParams(window.location.search);
    const sessionId = urlParams.get("session");
    
    // ========== æ–°å¢åŒè½¨å½•éŸ³å˜é‡ ==========
    let segmentRecorder = null;       // åˆ†æ®µå½•éŸ³å™¨
    let fullRecorder = null;           // å®Œæ•´å½•éŸ³å™¨
    let segmentChunks = [];            // å½“å‰åˆ†æ®µçš„éŸ³é¢‘å—
    let fullRecordingChunks = [];     // å®Œæ•´å½•éŸ³çš„éŸ³é¢‘å—
    let segmentStream = null;          // åˆ†æ®µå½•éŸ³æµ
    let fullStream = null;             // å®Œæ•´å½•éŸ³æµ
    
    // ========== ä¿®æ”¹åŸæœ‰å˜é‡ ==========
    let transcriptChunks = [];        // ä¿ç•™ï¼Œå­˜å‚¨æ‰€æœ‰è½¬å½•æ–‡æœ¬
    let segmentCount = 0;             // æ›¿ä»£åŸæ¥çš„recordingCycleCount
    let isRecording = false;          // æ›¿ä»£åŸæ¥çš„isManuallyStoppedé€»è¾‘
    let segmentTimer = null;          // æ–°å¢å®šæ—¶å™¨
    
    let previousSummary = {
      summary: '',
      decision: [],
      explicit: [],
      tacit: []
    };

    // UIå…ƒç´ 
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const textArea = document.getElementById('textArea');
    const recordingStatus = document.getElementById('recordingStatus');
    const segmentStatus = document.getElementById('segmentStatus');

    // ========== ä¸»è¦ä¿®æ”¹ï¼šå¯åŠ¨åŒè½¨å½•éŸ³ ==========
    startBtn.onclick = async () => {
      await startDualRecording();
    };
    
    stopBtn.onclick = async () => {
      await stopAllRecording();
    };

    // ========== æ–°å¢ï¼šåŒè½¨å½•éŸ³ç³»ç»Ÿ ==========
    async function startDualRecording() {
      try {
        console.log("ğŸ™ï¸ Starting dual-track recording...");
        
        // é‡ç½®çŠ¶æ€
        isRecording = true;
        sessionStartTime = Date.now();
        segmentCount = 0;
        transcriptChunks = [];
        segmentChunks = [];
        fullRecordingChunks = [];
        
        // è·å–ä¸¤ä¸ªç‹¬ç«‹çš„éŸ³é¢‘æµ
        segmentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        fullStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // è®¾ç½®åˆ†æ®µå½•éŸ³å™¨
        segmentRecorder = new MediaRecorder(segmentStream);
        segmentRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) segmentChunks.push(e.data);
        };
        
        // è®¾ç½®å®Œæ•´å½•éŸ³å™¨
        fullRecorder = new MediaRecorder(fullStream);
        fullRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) fullRecordingChunks.push(e.data);
        };
        
        // å¯åŠ¨ä¸¤ä¸ªå½•éŸ³å™¨
        segmentRecorder.start();
        fullRecorder.start();
        
        // UIæ›´æ–°
        startBtn.disabled = true;
        stopBtn.disabled = false;
        textArea.value = 'ğŸ™ï¸ Recording started (dual-track)...';
        recordingStatus.textContent = 'ğŸ”´ Recording in progress...';
        
        // å¯åŠ¨åˆ†æ®µå¤„ç†å®šæ—¶å™¨
        startSegmentTimer();
        
        console.log("âœ… Both recording tracks started");
        
      } catch (err) {
        console.error('âŒ Microphone error:', err);
        textArea.value = 'âŒ Microphone access denied.';
        resetUI();
      }
    }

    // ========== æ–°å¢ï¼šåˆ†æ®µå®šæ—¶å™¨ ==========
    function startSegmentTimer() {
      segmentTimer = setTimeout(async () => {
        if (isRecording) {
          await processSegment();
          startSegmentTimer(); // é€’å½’è°ƒç”¨
        }
      }, SEGMENT_DURATION);
    }

    // ========== ä¿®æ”¹ï¼šå¤„ç†åˆ†æ®µ ==========
    async function processSegment() {
      segmentCount++;
      console.log(`ğŸ“¦ Processing segment ${segmentCount}...`);
      segmentStatus.textContent = `Processing segment ${segmentCount}...`;
      
      // åœæ­¢å½“å‰åˆ†æ®µå½•éŸ³
      segmentRecorder.stop();
      
      // ç­‰å¾…æ•°æ®å¯ç”¨
      await new Promise(resolve => {
        segmentRecorder.onstop = resolve;
      });
      
      // åˆ›å»ºåˆ†æ®µblob
      const segmentBlob = new Blob(segmentChunks, { type: 'audio/webm' });
      segmentChunks = []; // é‡ç½®
      
      // å‘é€åˆ†æ®µè¿›è¡Œè½¬å½•
      await transcribeSegment(segmentBlob);
      
      // é‡å¯åˆ†æ®µå½•éŸ³å™¨
      if (isRecording) {
        segmentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        segmentRecorder = new MediaRecorder(segmentStream);
        segmentRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) segmentChunks.push(e.data);
        };
        segmentRecorder.start();
      }
      
      // æ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“
      if (segmentCount % SUMMARIZE_FREQUENCY === 0) {
        await performIntermediateSummary();
      }
    }

    // ========== ä¿®æ”¹ï¼šè½¬å½•åˆ†æ®µï¼ˆä½¿ç”¨AssemblyAIï¼‰ ==========
    async function transcribeSegment(blob) {
  const form = new FormData();
  form.append('file', blob, `segment_${segmentCount}.webm`);
  
  try {
    textArea.value = `ğŸ”„ Transcribing segment ${segmentCount} with speaker detection...`;
    
    const res = await fetch('https://fyp-2025-kath.vercel.app/api/analyze', {
      method: 'POST',
      body: form
    });
    
    const result = await res.json();
    
    if (result.success) {
      // å­˜å‚¨å¸¦è¯´è¯äººæ ‡ç­¾çš„è½¬å½•
      transcriptChunks.push(result.transcript);
      
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      // è®¡ç®—æ€»ç»“è¿›åº¦
      const currentProgress = segmentCount % SUMMARIZE_FREQUENCY;
      const segmentsUntilSummary = currentProgress === 0 ? 
        SUMMARIZE_FREQUENCY : 
        SUMMARIZE_FREQUENCY - currentProgress;
      const minutesUntilSummary = segmentsUntilSummary * 5;
      
      textArea.value = `â±ï¸ Recording time: ${duration} min\n` +
                      `ğŸ“ Segment ${segmentCount} transcribed\n` +
                      `ğŸ‘¥ Speakers detected: ${result.metadata?.speakers || 0}\n` +
                      `ğŸ“Š Progress: ${currentProgress || SUMMARIZE_FREQUENCY}/${SUMMARIZE_FREQUENCY} segments\n` +
                      `â­ï¸ Next summary: ${segmentsUntilSummary} segment${segmentsUntilSummary > 1 ? 's' : ''} (${minutesUntilSummary} min)\n\n` +
                      `Latest transcript:\n${result.transcript}`;
      
      segmentStatus.textContent = `âœ… Segment ${segmentCount} processed`;
    }
    
  } catch (err) {
    console.error('âŒ Segment transcription failed:', err);
    segmentStatus.textContent = `âŒ Segment ${segmentCount} failed`;
  }
}
    // ========== ä¿®æ”¹ï¼šä¸­é—´æ€»ç»“ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰ ==========
    async function performIntermediateSummary() {
      const combined = transcriptChunks.join('\n\n');
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      console.log('ğŸ“Š Generating intermediate summary...');
      textArea.value += '\n\nğŸ“Š Generating summary...';
      
      try {
        const avoidCombined = [
          previousSummary.summary,
          ...previousSummary.decision,
          ...previousSummary.explicit,
          ...previousSummary.tacit
        ].join('\n');

        const res = await fetch('https://fyp-2025-kath.vercel.app/api/summarize', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            text: combined,
            avoid: avoidCombined,
            session_id: sessionId
          })
        });
        
        const data = await res.json();
        
        if (data.summary) {
          // æ›´æ–°previousSummary
          previousSummary.summary += '\n' + data.summary;
          previousSummary.decision = [...previousSummary.decision, ...(data.decision || [])];
          previousSummary.explicit = [...previousSummary.explicit, ...(data.explicit || [])];
          previousSummary.tacit = [...previousSummary.tacit, ...(data.tacit || [])];
          
          textArea.value = `ğŸ“Š Summary (${duration} min):\n${data.summary}\n\n` +
                          `Continue recording...`;
          
          // ä¿å­˜ä¸­é—´æ€»ç»“
          await saveIntermediateSummary(data);
        }
        
      } catch (err) {
        console.error('âŒ Summary failed:', err);
      }
    }

    // ========== æ–°å¢ï¼šåœæ­¢æ‰€æœ‰å½•éŸ³å¹¶å¤„ç†å®Œæ•´å½•éŸ³ ==========
    async function stopAllRecording() {
      console.log('ğŸ›‘ Stopping all recording...');
      isRecording = false;
      
      // æ¸…é™¤å®šæ—¶å™¨
      if (segmentTimer) {
        clearTimeout(segmentTimer);
        segmentTimer = null;
      }
      
      // åœæ­¢åˆ†æ®µå½•éŸ³å™¨
      if (segmentRecorder && segmentRecorder.state === 'recording') {
        segmentRecorder.stop();
      }
      
      // åœæ­¢å®Œæ•´å½•éŸ³å™¨
      if (fullRecorder && fullRecorder.state === 'recording') {
        fullRecorder.stop();
        
        // ç­‰å¾…æ•°æ®
        await new Promise(resolve => {
          fullRecorder.onstop = resolve;
        });
      }
      
      // å¤„ç†å®Œæ•´å½•éŸ³
      await processFinalRecording();
      
      // é‡ç½®UI
      resetUI();
    }

    // ========== æ–°å¢ï¼šå¤„ç†å®Œæ•´å½•éŸ³ ==========
    async function processFinalRecording() {
      if (fullRecordingChunks.length === 0) {
        console.warn('No full recording data');
        return;
      }
      
      console.log('ğŸ¯ Processing complete recording...');
      textArea.value = 'ğŸ¯ Processing complete recording for final summary...\n' +
                      'This may take a few minutes...';
      
      const fullBlob = new Blob(fullRecordingChunks, { type: 'audio/webm' });
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      console.log(`ğŸ“¼ Full recording: ${(fullBlob.size / 1024 / 1024).toFixed(2)} MB, ${duration} min`);
      
      const form = new FormData();
      form.append('file', fullBlob, 'full_recording.webm');
      form.append('session_id', sessionId);
      form.append('duration_minutes', duration.toString());
      
      try {
        // å‘é€åˆ°æ–°çš„final-analyzeç«¯ç‚¹
        const res = await fetch('https://fyp-2025-kath.vercel.app/api/final-analyze', {
          method: 'POST',
          body: form
        });
        
        const result = await res.json();
        
        if (result.success) {
          textArea.value = `âœ¨ FINAL MEETING SUMMARY âœ¨\n` +
                          `Duration: ${duration} minutes\n` +
                          `Speakers: ${result.metadata?.speakers || 'Unknown'}\n\n` +
                          `ğŸ“ Complete Summary:\n${result.finalSummary}\n\n` +
                          `ğŸ¯ Decisions:\n${result.decisions?.join('\n') || 'None'}\n\n` +
                          `ğŸ’¡ Explicit Knowledge:\n${result.explicit?.join('\n') || 'None'}\n\n` +
                          `ğŸ’­ Tacit Knowledge:\n${result.tacit?.join('\n') || 'None'}`;
          
          // ä¿å­˜æœ€ç»ˆæ€»ç»“
          await saveFinalSummary(result);
        }
        
      } catch (err) {
        console.error('âŒ Final processing failed:', err);
        textArea.value += '\n\nâŒ Final processing failed.';
      }
    }

    // ========== ä¿å­˜å‡½æ•°ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰ ==========
    async function saveIntermediateSummary(summary) {
      try {
        await fetch('https://fyp-2025-kath.vercel.app/api/save', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            session: sessionId,
            transcript: transcriptChunks.join('\n'),
            summary: summary.summary,
            decision: summary.decision,
            explicit: summary.explicit,
            tacit: summary.tacit,
            reasoning: summary.reasoning,
            suggestions: summary.suggestions,
            is_intermediate: true,
            segment_count: segmentCount
          })
        });
      } catch (err) {
        console.error('Failed to save intermediate:', err);
      }
    }

    async function saveFinalSummary(result) {
      try {
        await fetch('https://fyp-2025-kath.vercel.app/api/save', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            session: sessionId,
            transcript: result.fullTranscript,
            summary: result.finalSummary,
            decision: result.decisions,
            explicit: result.explicit,
            tacit: result.tacit,
            reasoning: result.reasoning,
            suggestions: result.suggestions,
            is_final: true,
            duration_minutes: Math.floor((Date.now() - sessionStartTime) / 60000)
          })
        });
        console.log('âœ… Final summary saved');
      } catch (err) {
        console.error('Failed to save final:', err);
      }
    }

    // ========== UIè¾…åŠ©å‡½æ•° ==========
    function resetUI() {
      startBtn.disabled = false;
      stopBtn.disabled = true;
      recordingStatus.textContent = '';
      segmentStatus.textContent = '';
      
      // åœæ­¢æ‰€æœ‰æµ
      if (segmentStream) {
        segmentStream.getTracks().forEach(track => track.stop());
      }
      if (fullStream) {
        fullStream.getTracks().forEach(track => track.stop());
      }
    }
  </script>
</body>
</html>