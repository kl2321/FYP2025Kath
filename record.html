<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <style>
    body { font-family: sans-serif; padding: 16px; }
    button { padding: 8px 16px; margin-right: 8px; }
    textarea { width: 100%; height: 200px; margin-top: 12px; white-space: pre-wrap; }
    /* æ–°å¢æ ·å¼ */
    .status-panel {
      background: #f0f0f0;
      padding: 10px;
      border-radius: 5px;
      margin: 10px 0;
      font-size: 14px;
    }
    .status-line {
      margin: 5px 0;
    }
  </style>
</head>
<body>
 
<div class="config-info" style="background: #d4edda; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
  <strong>Recording Configuration:</strong><br>
  Processing: Every 5 minutes<br>
  Auto-summarize: Every <span id="intervalMin"></span> minutes (after <span id="segmentsNeeded"></span> segments)
</div>


  <h2>ğŸ™ï¸ Record Meeting</h2>
  <div class="status-panel" style="position: fixed; top: 10px; right: 10px; background: white; padding: 10px; border: 1px solid #ddd; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
  <div id="recordingTime">â±ï¸ Not recording</div>
  <div id="audioLevel">ğŸ¤ No audio</div>
  <div id="segmentProgress">ğŸ“Š Waiting...</div>
</div>
   <div class="status-panel">
    <div class="status-line" id="recordingStatus"></div>
    <div class="status-line" id="segmentStatus"></div>
  </div>
  
  <button id="startBtn">Start</button>
  <button id="stopBtn" disabled>Stop</button>
  
  <textarea id="textArea" placeholder="Transcript and summary will appear here..."></textarea>

  <script>
    
    const urlParams = new URLSearchParams(window.location.search);
    const sessionId = urlParams.get("session");
    // ========== é…ç½®å¸¸é‡ - ä¿®æ”¹ä¸º3åˆ†é’Ÿ ==========
    const SEGMENT_DURATION = 5 * 60 * 1000;  // æ”¹ä¸º3åˆ†é’Ÿ (åŸæœ¬æ˜¯30ç§’)
    const intervalMin = parseInt(urlParams.get("intervalMin") || "10");  // é»˜è®¤10åˆ†é’Ÿ
    const SUMMARIZE_FREQUENCY = Math.ceil(intervalMin / 5);  // è®¡ç®—éœ€è¦å‡ ä¸ªsegments

    window.onload = function() {
    const intervalElement = document.getElementById('intervalMin');
    const segmentsElement = document.getElementById('segmentsNeeded');
    
    if (intervalElement) intervalElement.textContent = intervalMin;
    if (segmentsElement) segmentsElement.textContent = SUMMARIZE_FREQUENCY;
  };

  console.log("ğŸ“Š Configuration:", {
    sessionId,
    intervalMin,
    segmentsNeeded: SUMMARIZE_FREQUENCY
  });
   
    
    // ========== ä¿ç•™åŸæœ‰çš„å˜é‡ ==========
    let contextPdfText = "";
    let sessionStartTime = null;
    
   
    
    // ========== æ–°å¢åŒè½¨å½•éŸ³å˜é‡ ==========
    let segmentRecorder = null;       // åˆ†æ®µå½•éŸ³å™¨
    let fullRecorder = null;           // å®Œæ•´å½•éŸ³å™¨
    let segmentChunks = [];            // å½“å‰åˆ†æ®µçš„éŸ³é¢‘å—
    let fullRecordingChunks = [];     // å®Œæ•´å½•éŸ³çš„éŸ³é¢‘å—
    let segmentStream = null;          // åˆ†æ®µå½•éŸ³æµ
    let fullStream = null;             // å®Œæ•´å½•éŸ³æµ
    
    // ========== ä¿®æ”¹åŸæœ‰å˜é‡ ==========
    let transcriptChunks = [];        // ä¿ç•™ï¼Œå­˜å‚¨æ‰€æœ‰è½¬å½•æ–‡æœ¬
    let segmentCount = 0;             // æ›¿ä»£åŸæ¥çš„recordingCycleCount
    let isRecording = false;          // æ›¿ä»£åŸæ¥çš„isManuallyStoppedé€»è¾‘
    let segmentTimer = null;          // æ–°å¢å®šæ—¶å™¨
    
    let previousSummary = {
      summary: '',
      decision: [],
      explicit: [],
      tacit: []
    };

    // UIå…ƒç´ 
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const textArea = document.getElementById('textArea');
    const recordingStatus = document.getElementById('recordingStatus');
    const segmentStatus = document.getElementById('segmentStatus');

    // ========== ä¸»è¦ä¿®æ”¹ï¼šå¯åŠ¨åŒè½¨å½•éŸ³ ==========
    startBtn.onclick = async () => {
      await startDualRecording();
    };
    
    stopBtn.onclick = async () => {
      await stopAllRecording();
    };

    // ========== æ–°å¢ï¼šåŒè½¨å½•éŸ³ç³»ç»Ÿ ==========
    async function startDualRecording() {
      try {
        console.log("ğŸ™ï¸ Starting dual-track recording...");
        // å®‰å…¨æ£€æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨
    const recordingStatus = document.getElementById('recordingStatus');
    const segmentStatus = document.getElementById('segmentStatus');
    
    if (!recordingStatus || !segmentStatus) {
      console.error('âŒ Status elements not found in DOM');
      // åˆ›å»ºç¼ºå¤±çš„å…ƒç´ 
      if (!recordingStatus) {
        console.warn('Creating missing recordingStatus element');
      }
      if (!segmentStatus) {
        console.warn('Creating missing segmentStatus element');
      }
    }
        
        // é‡ç½®çŠ¶æ€
        isRecording = true;
        sessionStartTime = Date.now();
        segmentCount = 0;
        transcriptChunks = [];
        segmentChunks = [];
        fullRecordingChunks = [];
        
        // è·å–ä¸¤ä¸ªç‹¬ç«‹çš„éŸ³é¢‘æµ
        segmentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        fullStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // è®¾ç½®åˆ†æ®µå½•éŸ³å™¨
        segmentRecorder = new MediaRecorder(segmentStream, {
          mimeType: 'audio/webm;codecs=opus',
          audioBitsPerSecond: 8000  // æä½æ¯”ç‰¹ç‡ 8kbps
        });
        segmentRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) segmentChunks.push(e.data);
        };
        
        // è®¾ç½®å®Œæ•´å½•éŸ³å™¨
        fullRecorder = new MediaRecorder(fullStream, {
          mimeType: 'audio/webm;codecs=opus',
          audioBitsPerSecond: 8000  // 8kbps
        });
        fullRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) fullRecordingChunks.push(e.data);
        };
        
        // å¯åŠ¨ä¸¤ä¸ªå½•éŸ³å™¨
        segmentRecorder.start();
        fullRecorder.start();
        
        // UIæ›´æ–°
        startBtn.disabled = true;
        stopBtn.disabled = false;
        textArea.value = 'ğŸ™ï¸ Recording started (dual-track)...';
         if (recordingStatus) {
      recordingStatus.textContent = 'ğŸ”´ Recording in progress...';
    }
        
        // å¯åŠ¨åˆ†æ®µå¤„ç†å®šæ—¶å™¨
        startSegmentTimer();
        
        console.log("âœ… Both recording tracks started");
        
      } catch (err) {
        console.error('âŒ Microphone error:', err);
        textArea.value = 'âŒ Microphone access denied.';
        resetUI();
      }
    }
    let recordingTimer = null;
    function updateRecordingTime() {
  if (!isRecording || !sessionStartTime) return;
  
  const elapsed = Date.now() - sessionStartTime;
  const minutes = Math.floor(elapsed / 60000);
  const seconds = Math.floor((elapsed % 60000) / 1000);
  
  // è®¡ç®—ä¸‹ä¸€ä¸ªsegmentçš„å‰©ä½™æ—¶é—´
  const nextSegmentIn = SEGMENT_DURATION - (elapsed % SEGMENT_DURATION);
  const nextMinutes = Math.floor(nextSegmentIn / 60000);
  const nextSeconds = Math.floor((nextSegmentIn % 60000) / 1000);
  
  recordingStatus.innerHTML = `
    ğŸ”´ Recording: ${minutes}:${seconds.toString().padStart(2, '0')}<br>
    â­ï¸ Next segment in: ${nextMinutes}:${nextSeconds.toString().padStart(2, '0')}
  `;
}

// åœ¨ startDualRecording æˆåŠŸåå¯åŠ¨è®¡æ—¶å™¨
recordingTimer = setInterval(updateRecordingTime, 1000);
// æ¯30ç§’è¾“å‡ºä¸€æ¬¡çŠ¶æ€ï¼ˆç”¨äºæµ‹è¯•ï¼‰
setInterval(() => {
  if (isRecording) {
    console.log(`ğŸ“Š Status at ${new Date().toLocaleTimeString()}:`, {
      recording: true,
      segments: segmentCount,
      duration: Math.floor((Date.now() - sessionStartTime) / 60000) + ' min',
      nextSegmentIn: Math.ceil((SEGMENT_DURATION - ((Date.now() - sessionStartTime) % SEGMENT_DURATION)) / 1000) + ' sec'
    });
  }
}, 30000);
// æ·»åŠ éŸ³é¢‘æ´»åŠ¨æ£€æµ‹
async function checkAudioActivity() {
  if (!segmentStream) return;
  
  const audioContext = new AudioContext();
  const analyser = audioContext.createAnalyser();
  const microphone = audioContext.createMediaStreamSource(segmentStream);
  const dataArray = new Uint8Array(analyser.frequencyBinCount);
  
  microphone.connect(analyser);
  
  function checkLevel() {
    analyser.getByteFrequencyData(dataArray);
    const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
    
    // æ˜¾ç¤ºéŸ³é¢‘çº§åˆ«
    const level = Math.min(100, Math.floor(average));
    const bars = 'â–ˆ'.repeat(Math.floor(level / 10));
    
    if (document.getElementById('audioLevel')) {
      document.getElementById('audioLevel').textContent = `Audio: ${bars} ${level}%`;
    }
    
    if (isRecording) {
      requestAnimationFrame(checkLevel);
    }
  }
  
  checkLevel();
}

    // ========== æ–°å¢ï¼šåˆ†æ®µå®šæ—¶å™¨ ==========
    function startSegmentTimer() {
      segmentTimer = setTimeout(async () => {
        if (isRecording) {
          await processSegment();
          startSegmentTimer(); // é€’å½’è°ƒç”¨
        }
      }, SEGMENT_DURATION);
    }

    // ========== ä¿®æ”¹ï¼šå¤„ç†åˆ†æ®µ ==========
    async function processSegment() {
      segmentCount++;
      console.log(`ğŸ“¦ Processing segment ${segmentCount}...`);
      segmentStatus.textContent = `Processing segment ${segmentCount}...`;
      
      // åœæ­¢å½“å‰åˆ†æ®µå½•éŸ³
      segmentRecorder.stop();
      
      // ç­‰å¾…æ•°æ®å¯ç”¨
      await new Promise(resolve => {
        segmentRecorder.onstop = resolve;
      });
      
      // åˆ›å»ºåˆ†æ®µblob
      const segmentBlob = new Blob(segmentChunks, { type: 'audio/webm' });

      // æ£€æŸ¥æ–‡ä»¶å¤§å°
  const sizeMB = segmentBlob.size / 1024 / 1024;
  console.log(`ğŸ“Š Segment ${segmentCount} size: ${sizeMB.toFixed(2)} MB`);
  
  if (sizeMB > 40) {
    console.error(`âŒ Segment too large: ${sizeMB.toFixed(2)} MB`);
    segmentStatus.textContent = `âŒ Segment ${segmentCount} too large`;
    segmentChunks = []; // æ¸…ç©º
    // ç»§ç»­å½•éŸ³ä½†è·³è¿‡è¿™ä¸ªsegment
  } else {
    await transcribeSegment(segmentBlob);
  }
      segmentChunks = []; // é‡ç½®
      
      // å‘é€åˆ†æ®µè¿›è¡Œè½¬å½•
      await transcribeSegment(segmentBlob);
      
      // é‡å¯åˆ†æ®µå½•éŸ³å™¨
      if (isRecording) {
        segmentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        segmentRecorder = new MediaRecorder(segmentStream , {
          mimeType: 'audio/webm;codecs=opus',
          audioBitsPerSecond: 8000  // ç¡®ä¿ä½æ¯”ç‰¹ç‡
        });
        segmentRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) segmentChunks.push(e.data);
        };
        segmentRecorder.start();
      }
      
      // æ£€æŸ¥æ˜¯å¦éœ€è¦æ€»ç»“
      if (segmentCount % SUMMARIZE_FREQUENCY === 0) {
        await performIntermediateSummary();
      }
    }

    // ========== ä¿®æ”¹ï¼šè½¬å½•åˆ†æ®µï¼ˆä½¿ç”¨AssemblyAIï¼‰ ==========
    async function transcribeSegment(blob) {
      // æ˜¾ç¤ºæ–‡ä»¶å¤§å°
  const sizeMB = blob.size / 1024 / 1024;
  console.log(`ğŸ“ Uploading segment ${segmentCount}: ${sizeMB.toFixed(2)} MB`);
  
  if (sizeMB > 45) {
    throw new Error(`File too large: ${sizeMB.toFixed(2)} MB (max 45 MB)`);
  }
  const form = new FormData();
  form.append('file', blob, `segment_${segmentCount}.webm`);
  
  try {
    textArea.value = `ğŸ”„ Transcribing segment ${segmentCount} with speaker detection...`;
    
    const res = await fetch('https://fyp-2025-kath.vercel.app/api/analyze', {
      method: 'POST',
      body: form
    });

    // å…ˆæ£€æŸ¥å“åº”çŠ¶æ€
    if (!res.ok) {
      const errorText = await res.text();
      console.error('âŒ API error:', res.status, errorText);
      throw new Error(`API returned ${res.status}: ${errorText}`);
    }

    // è·å–å“åº”æ–‡æœ¬
    const responseText = await res.text();
    console.log('ğŸ“¥ Raw response:', responseText.substring(0, 100)); // åªæ˜¾ç¤ºå‰100å­—ç¬¦
    
    
  // å°è¯•è§£æJSON
    let result;
    try {
      result = JSON.parse(responseText);
    } catch (parseError) {
      console.error('âŒ JSON parse error:', parseError);
      console.error('Response was:', responseText);
      throw new Error('Invalid JSON response from API');
    }
    
    if (result.success) {
      // å­˜å‚¨å¸¦è¯´è¯äººæ ‡ç­¾çš„è½¬å½•
      transcriptChunks.push(result.transcript);
      
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      // è®¡ç®—æ€»ç»“è¿›åº¦
      const currentProgress = segmentCount % SUMMARIZE_FREQUENCY;
      const segmentsUntilSummary = currentProgress === 0 ? 
        SUMMARIZE_FREQUENCY : 
        SUMMARIZE_FREQUENCY - currentProgress;
      const minutesUntilSummary = segmentsUntilSummary * 5;
      
      textArea.value = `â±ï¸ Recording time: ${duration} min\n` +
                      `ğŸ“ Sgment ${segmentCount} transcribed\n` +
                      `ğŸ‘¥ Speakers detected: ${result.metadata?.speakers || 0}\n` +
                      `ğŸ“Š Progress: ${currentProgress || SUMMARIZE_FREQUENCY}/${SUMMARIZE_FREQUENCY} segments\n` +
                      `â­ï¸ Next summary: ${segmentsUntilSummary} segment${segmentsUntilSummary > 1 ? 's' : ''} (${minutesUntilSummary} min)\n\n` +
                      `Latest transcript:\n${result.transcript}`;
      
      segmentStatus.textContent = `âœ… Segment ${segmentCount} processed`;
    } else {
      throw new Error(result.error || 'Transcription failed');
    }
    
  } catch (err) {
    console.error('âŒ Segment transcription failed:', err);
    segmentStatus.textContent = `âŒ Segment ${segmentCount} failed: ${err.message}`;
    // æ˜¾ç¤ºé”™è¯¯ä½†ç»§ç»­å½•éŸ³
    textArea.value = `âŒ Segment ${segmentCount} failed to transcribe\n` +
                    `Error: ${err.message}\n` +
                    `Continuing recording...\n\n` +
                    textArea.value;
  }
}
    // ========== ä¿®æ”¹ï¼šä¸­é—´æ€»ç»“ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰ ==========
    async function performIntermediateSummary() {
      const combined = transcriptChunks.join('\n\n');
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      console.log('ğŸ“Š Generating intermediate summary...');
      textArea.value += '\n\nğŸ“Š Generating summary...';
      
      try {
        const avoidCombined = [
          previousSummary.summary,
          ...previousSummary.decision,
          ...previousSummary.explicit,
          ...previousSummary.tacit
        ].join('\n');

        const res = await fetch('https://fyp-2025-kath.vercel.app/api/summarize', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            text: combined,
            avoid: avoidCombined,
            session_id: sessionId
          })
        });
        
        const data = await res.json();
        
        if (data.summary) {
          // æ›´æ–°previousSummary
          previousSummary.summary += '\n' + data.summary;
          previousSummary.decision = [...previousSummary.decision, ...(data.decision || [])];
          previousSummary.explicit = [...previousSummary.explicit, ...(data.explicit || [])];
          previousSummary.tacit = [...previousSummary.tacit, ...(data.tacit || [])];
          
          textArea.value = `ğŸ“Š Summary (${duration} min):\n${data.summary}\n\n` +
                          `Continue recording...`;
          
          // ä¿å­˜ä¸­é—´æ€»ç»“
          await saveIntermediateSummary(data);
        }
        
      } catch (err) {
        console.error('âŒ Summary failed:', err);
      }
    }

    // ========== æ–°å¢ï¼šåœæ­¢æ‰€æœ‰å½•éŸ³å¹¶å¤„ç†å®Œæ•´å½•éŸ³ ==========
    async function stopAllRecording() {
      console.log('ğŸ›‘ Stopping all recording...');
      isRecording = false;
      
      // æ¸…é™¤å®šæ—¶å™¨
      if (segmentTimer) {
        clearTimeout(segmentTimer);
        segmentTimer = null;
      }
      
      // åœæ­¢åˆ†æ®µå½•éŸ³å™¨
      if (segmentRecorder && segmentRecorder.state === 'recording') {
        segmentRecorder.stop();
      }
      
      // åœæ­¢å®Œæ•´å½•éŸ³å™¨
      if (fullRecorder && fullRecorder.state === 'recording') {
        fullRecorder.stop();
        
        // ç­‰å¾…æ•°æ®
        await new Promise(resolve => {
          fullRecorder.onstop = resolve;
        });
      }
      
      // å¤„ç†å®Œæ•´å½•éŸ³
      await processFinalRecording();
      
      // é‡ç½®UI
      resetUI();
    }

    // ========== æ–°å¢ï¼šå¤„ç†å®Œæ•´å½•éŸ³ ==========
    async function processFinalRecording() {
      if (fullRecordingChunks.length === 0) {
        console.warn('No full recording data');
        return;
      }
      
      console.log('ğŸ¯ Processing complete recording...');
      textArea.value = 'ğŸ¯ Processing complete recording for final summary...\n' +
                      'This may take a few minutes...';
      
      const fullBlob = new Blob(fullRecordingChunks, { type: 'audio/webm' });
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      console.log(`ğŸ“¼ Full recording: ${(fullBlob.size / 1024 / 1024).toFixed(2)} MB, ${duration} min`);
      
      const form = new FormData();
      form.append('file', fullBlob, 'full_recording.webm');
      form.append('session_id', sessionId);
      form.append('duration_minutes', duration.toString());
      
      try {
        // å‘é€åˆ°æ–°çš„final-analyzeç«¯ç‚¹
        const res = await fetch('https://fyp-2025-kath.vercel.app/api/final-analyze', {
          method: 'POST',
          body: form
        });
        
        const result = await res.json();
        
        if (result.success) {
          textArea.value = `âœ¨ FINAL MEETING SUMMARY âœ¨\n` +
                          `Duration: ${duration} minutes\n` +
                          `Speakers: ${result.metadata?.speakers || 'Unknown'}\n\n` +
                          `ğŸ“ Complete Summary:\n${result.finalSummary}\n\n` +
                          `ğŸ¯ Decisions:\n${result.decisions?.join('\n') || 'None'}\n\n` +
                          `ğŸ’¡ Explicit Knowledge:\n${result.explicit?.join('\n') || 'None'}\n\n` +
                          `ğŸ’­ Tacit Knowledge:\n${result.tacit?.join('\n') || 'None'}`;
          
          // ä¿å­˜æœ€ç»ˆæ€»ç»“
          await saveFinalSummary(result);
        }
        
      } catch (err) {
        console.error('âŒ Final processing failed:', err);
        textArea.value += '\n\nâŒ Final processing failed.';
      }
    }

    // ========== ä¿å­˜å‡½æ•°ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰ ==========
    async function saveIntermediateSummary(summary) {
      try {
        await fetch('https://fyp-2025-kath.vercel.app/api/save', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            session: sessionId,
            transcript: transcriptChunks.join('\n'),
            summary: summary.summary,
            decision: summary.decision,
            explicit: summary.explicit,
            tacit: summary.tacit,
            reasoning: summary.reasoning,
            suggestions: summary.suggestions,
            is_intermediate: true,
            segment_count: segmentCount
          })
        });
      } catch (err) {
        console.error('Failed to save intermediate:', err);
      }
    }

    async function saveFinalSummary(result) {
      try {
        await fetch('https://fyp-2025-kath.vercel.app/api/save', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            session: sessionId,
            transcript: result.fullTranscript,
            summary: result.finalSummary,
            decision: result.decisions,
            explicit: result.explicit,
            tacit: result.tacit,
            reasoning: result.reasoning,
            suggestions: result.suggestions,
            is_final: true,
            duration_minutes: Math.floor((Date.now() - sessionStartTime) / 60000)
          })
        });
        console.log('âœ… Final summary saved');
      } catch (err) {
        console.error('Failed to save final:', err);
      }
    }

    // ========== UIè¾…åŠ©å‡½æ•° ==========
    function resetUI() {
      startBtn.disabled = false;
      stopBtn.disabled = true;
      recordingStatus.textContent = '';
      segmentStatus.textContent = '';
      
      // åœæ­¢æ‰€æœ‰æµ
      if (segmentStream) {
        segmentStream.getTracks().forEach(track => track.stop());
      }
      if (fullStream) {
        fullStream.getTracks().forEach(track => track.stop());
      }
    }
  </script>
</body>
</html>