<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <style>
    body { font-family: sans-serif; padding: 16px; }
    button { padding: 8px 16px; margin-right: 8px; }
    textarea { width: 100%; height: 200px; margin-top: 12px; white-space: pre-wrap; }
    /* 新增样式 */
    .status-panel {
      background: #f0f0f0;
      padding: 10px;
      border-radius: 5px;
      margin: 10px 0;
      font-size: 14px;
    }
    .status-line {
      margin: 5px 0;
    }
  </style>
</head>
<body>
  // 在状态面板前添加配置信息显示
<div class="config-info" style="background: #d4edda; padding: 10px; border-radius: 5px; margin-bottom: 10px;">
  <strong>Recording Configuration:</strong><br>
  Processing: Every 5 minutes<br>
  Auto-summarize: Every <span id="summaryMinutes"></span> minutes (after <span id="segmentsNeeded"></span> segments)
</div>

// 在script开始处添加：
document.getElementById('intervalMin').textContent = intervalMin;
document.getElementById('segmentsNeeded').textContent = SUMMARIZE_FREQUENCY;
  <h2>🎙️ Record Meeting</h2>
  
  <!-- 新增状态面板 -->
  <div class="status-panel">
    <div class="status-line" id="recordingStatus"></div>
    <div class="status-line" id="segmentStatus"></div>
  </div>
  
  <button id="startBtn">Start</button>
  <button id="stopBtn" disabled>Stop</button>
  
  <textarea id="textArea" placeholder="Transcript and summary will appear here..."></textarea>

  <script>
    // ========== 配置常量 - 修改为3分钟 ==========
    const SEGMENT_DURATION = 5 * 60 * 1000;  // 改为3分钟 (原本是30秒)
    const intervalMin = parseInt(urlParams.get("intervalMin") || "10");  // 默认10分钟
    const SUMMARIZE_FREQUENCY = Math.ceil(intervalMin / 5);  // 计算需要几个segments
   
    
    // ========== 保留原有的变量 ==========
    let contextPdfText = "";
    let sessionStartTime = null;
    const urlParams = new URLSearchParams(window.location.search);
    const sessionId = urlParams.get("session");
    
    // ========== 新增双轨录音变量 ==========
    let segmentRecorder = null;       // 分段录音器
    let fullRecorder = null;           // 完整录音器
    let segmentChunks = [];            // 当前分段的音频块
    let fullRecordingChunks = [];     // 完整录音的音频块
    let segmentStream = null;          // 分段录音流
    let fullStream = null;             // 完整录音流
    
    // ========== 修改原有变量 ==========
    let transcriptChunks = [];        // 保留，存储所有转录文本
    let segmentCount = 0;             // 替代原来的recordingCycleCount
    let isRecording = false;          // 替代原来的isManuallyStopped逻辑
    let segmentTimer = null;          // 新增定时器
    
    let previousSummary = {
      summary: '',
      decision: [],
      explicit: [],
      tacit: []
    };

    // UI元素
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const textArea = document.getElementById('textArea');
    const recordingStatus = document.getElementById('recordingStatus');
    const segmentStatus = document.getElementById('segmentStatus');

    // ========== 主要修改：启动双轨录音 ==========
    startBtn.onclick = async () => {
      await startDualRecording();
    };
    
    stopBtn.onclick = async () => {
      await stopAllRecording();
    };

    // ========== 新增：双轨录音系统 ==========
    async function startDualRecording() {
      try {
        console.log("🎙️ Starting dual-track recording...");
        
        // 重置状态
        isRecording = true;
        sessionStartTime = Date.now();
        segmentCount = 0;
        transcriptChunks = [];
        segmentChunks = [];
        fullRecordingChunks = [];
        
        // 获取两个独立的音频流
        segmentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        fullStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        
        // 设置分段录音器
        segmentRecorder = new MediaRecorder(segmentStream);
        segmentRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) segmentChunks.push(e.data);
        };
        
        // 设置完整录音器
        fullRecorder = new MediaRecorder(fullStream);
        fullRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) fullRecordingChunks.push(e.data);
        };
        
        // 启动两个录音器
        segmentRecorder.start();
        fullRecorder.start();
        
        // UI更新
        startBtn.disabled = true;
        stopBtn.disabled = false;
        textArea.value = '🎙️ Recording started (dual-track)...';
        recordingStatus.textContent = '🔴 Recording in progress...';
        
        // 启动分段处理定时器
        startSegmentTimer();
        
        console.log("✅ Both recording tracks started");
        
      } catch (err) {
        console.error('❌ Microphone error:', err);
        textArea.value = '❌ Microphone access denied.';
        resetUI();
      }
    }

    // ========== 新增：分段定时器 ==========
    function startSegmentTimer() {
      segmentTimer = setTimeout(async () => {
        if (isRecording) {
          await processSegment();
          startSegmentTimer(); // 递归调用
        }
      }, SEGMENT_DURATION);
    }

    // ========== 修改：处理分段 ==========
    async function processSegment() {
      segmentCount++;
      console.log(`📦 Processing segment ${segmentCount}...`);
      segmentStatus.textContent = `Processing segment ${segmentCount}...`;
      
      // 停止当前分段录音
      segmentRecorder.stop();
      
      // 等待数据可用
      await new Promise(resolve => {
        segmentRecorder.onstop = resolve;
      });
      
      // 创建分段blob
      const segmentBlob = new Blob(segmentChunks, { type: 'audio/webm' });
      segmentChunks = []; // 重置
      
      // 发送分段进行转录
      await transcribeSegment(segmentBlob);
      
      // 重启分段录音器
      if (isRecording) {
        segmentStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        segmentRecorder = new MediaRecorder(segmentStream);
        segmentRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) segmentChunks.push(e.data);
        };
        segmentRecorder.start();
      }
      
      // 检查是否需要总结
      if (segmentCount % SUMMARIZE_FREQUENCY === 0) {
        await performIntermediateSummary();
      }
    }

    // ========== 修改：转录分段（使用AssemblyAI） ==========
    async function transcribeSegment(blob) {
  const form = new FormData();
  form.append('file', blob, `segment_${segmentCount}.webm`);
  
  try {
    textArea.value = `🔄 Transcribing segment ${segmentCount} with speaker detection...`;
    
    const res = await fetch('https://fyp-2025-kath.vercel.app/api/analyze', {
      method: 'POST',
      body: form
    });
    
    const result = await res.json();
    
    if (result.success) {
      // 存储带说话人标签的转录
      transcriptChunks.push(result.transcript);
      
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      // 计算总结进度
      const currentProgress = segmentCount % SUMMARIZE_FREQUENCY;
      const segmentsUntilSummary = currentProgress === 0 ? 
        SUMMARIZE_FREQUENCY : 
        SUMMARIZE_FREQUENCY - currentProgress;
      const minutesUntilSummary = segmentsUntilSummary * 5;
      
      textArea.value = `⏱️ Recording time: ${duration} min\n` +
                      `📝 Segment ${segmentCount} transcribed\n` +
                      `👥 Speakers detected: ${result.metadata?.speakers || 0}\n` +
                      `📊 Progress: ${currentProgress || SUMMARIZE_FREQUENCY}/${SUMMARIZE_FREQUENCY} segments\n` +
                      `⏭️ Next summary: ${segmentsUntilSummary} segment${segmentsUntilSummary > 1 ? 's' : ''} (${minutesUntilSummary} min)\n\n` +
                      `Latest transcript:\n${result.transcript}`;
      
      segmentStatus.textContent = `✅ Segment ${segmentCount} processed`;
    }
    
  } catch (err) {
    console.error('❌ Segment transcription failed:', err);
    segmentStatus.textContent = `❌ Segment ${segmentCount} failed`;
  }
}
    // ========== 修改：中间总结（保持原有逻辑） ==========
    async function performIntermediateSummary() {
      const combined = transcriptChunks.join('\n\n');
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      console.log('📊 Generating intermediate summary...');
      textArea.value += '\n\n📊 Generating summary...';
      
      try {
        const avoidCombined = [
          previousSummary.summary,
          ...previousSummary.decision,
          ...previousSummary.explicit,
          ...previousSummary.tacit
        ].join('\n');

        const res = await fetch('https://fyp-2025-kath.vercel.app/api/summarize', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            text: combined,
            avoid: avoidCombined,
            session_id: sessionId
          })
        });
        
        const data = await res.json();
        
        if (data.summary) {
          // 更新previousSummary
          previousSummary.summary += '\n' + data.summary;
          previousSummary.decision = [...previousSummary.decision, ...(data.decision || [])];
          previousSummary.explicit = [...previousSummary.explicit, ...(data.explicit || [])];
          previousSummary.tacit = [...previousSummary.tacit, ...(data.tacit || [])];
          
          textArea.value = `📊 Summary (${duration} min):\n${data.summary}\n\n` +
                          `Continue recording...`;
          
          // 保存中间总结
          await saveIntermediateSummary(data);
        }
        
      } catch (err) {
        console.error('❌ Summary failed:', err);
      }
    }

    // ========== 新增：停止所有录音并处理完整录音 ==========
    async function stopAllRecording() {
      console.log('🛑 Stopping all recording...');
      isRecording = false;
      
      // 清除定时器
      if (segmentTimer) {
        clearTimeout(segmentTimer);
        segmentTimer = null;
      }
      
      // 停止分段录音器
      if (segmentRecorder && segmentRecorder.state === 'recording') {
        segmentRecorder.stop();
      }
      
      // 停止完整录音器
      if (fullRecorder && fullRecorder.state === 'recording') {
        fullRecorder.stop();
        
        // 等待数据
        await new Promise(resolve => {
          fullRecorder.onstop = resolve;
        });
      }
      
      // 处理完整录音
      await processFinalRecording();
      
      // 重置UI
      resetUI();
    }

    // ========== 新增：处理完整录音 ==========
    async function processFinalRecording() {
      if (fullRecordingChunks.length === 0) {
        console.warn('No full recording data');
        return;
      }
      
      console.log('🎯 Processing complete recording...');
      textArea.value = '🎯 Processing complete recording for final summary...\n' +
                      'This may take a few minutes...';
      
      const fullBlob = new Blob(fullRecordingChunks, { type: 'audio/webm' });
      const duration = Math.floor((Date.now() - sessionStartTime) / 60000);
      
      console.log(`📼 Full recording: ${(fullBlob.size / 1024 / 1024).toFixed(2)} MB, ${duration} min`);
      
      const form = new FormData();
      form.append('file', fullBlob, 'full_recording.webm');
      form.append('session_id', sessionId);
      form.append('duration_minutes', duration.toString());
      
      try {
        // 发送到新的final-analyze端点
        const res = await fetch('https://fyp-2025-kath.vercel.app/api/final-analyze', {
          method: 'POST',
          body: form
        });
        
        const result = await res.json();
        
        if (result.success) {
          textArea.value = `✨ FINAL MEETING SUMMARY ✨\n` +
                          `Duration: ${duration} minutes\n` +
                          `Speakers: ${result.metadata?.speakers || 'Unknown'}\n\n` +
                          `📝 Complete Summary:\n${result.finalSummary}\n\n` +
                          `🎯 Decisions:\n${result.decisions?.join('\n') || 'None'}\n\n` +
                          `💡 Explicit Knowledge:\n${result.explicit?.join('\n') || 'None'}\n\n` +
                          `💭 Tacit Knowledge:\n${result.tacit?.join('\n') || 'None'}`;
          
          // 保存最终总结
          await saveFinalSummary(result);
        }
        
      } catch (err) {
        console.error('❌ Final processing failed:', err);
        textArea.value += '\n\n❌ Final processing failed.';
      }
    }

    // ========== 保存函数（保持原有逻辑） ==========
    async function saveIntermediateSummary(summary) {
      try {
        await fetch('https://fyp-2025-kath.vercel.app/api/save', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            session: sessionId,
            transcript: transcriptChunks.join('\n'),
            summary: summary.summary,
            decision: summary.decision,
            explicit: summary.explicit,
            tacit: summary.tacit,
            reasoning: summary.reasoning,
            suggestions: summary.suggestions,
            is_intermediate: true,
            segment_count: segmentCount
          })
        });
      } catch (err) {
        console.error('Failed to save intermediate:', err);
      }
    }

    async function saveFinalSummary(result) {
      try {
        await fetch('https://fyp-2025-kath.vercel.app/api/save', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            session: sessionId,
            transcript: result.fullTranscript,
            summary: result.finalSummary,
            decision: result.decisions,
            explicit: result.explicit,
            tacit: result.tacit,
            reasoning: result.reasoning,
            suggestions: result.suggestions,
            is_final: true,
            duration_minutes: Math.floor((Date.now() - sessionStartTime) / 60000)
          })
        });
        console.log('✅ Final summary saved');
      } catch (err) {
        console.error('Failed to save final:', err);
      }
    }

    // ========== UI辅助函数 ==========
    function resetUI() {
      startBtn.disabled = false;
      stopBtn.disabled = true;
      recordingStatus.textContent = '';
      segmentStatus.textContent = '';
      
      // 停止所有流
      if (segmentStream) {
        segmentStream.getTracks().forEach(track => track.stop());
      }
      if (fullStream) {
        fullStream.getTracks().forEach(track => track.stop());
      }
    }
  </script>
</body>
</html>