// import { IncomingForm } from 'formidable';
// import fs from 'fs';
// import axios from 'axios';
// import FormData from 'form-data';

// export const config = {
//   api: {
//     bodyParser: false,
//   },
// };

// export default async function handler(req, res) {
//   if (req.method !== 'POST') {
//     return res.status(405).json({ error: 'Method not allowed' });
//   }

//   const form = new IncomingForm({
//     uploadDir: '/tmp',
//     keepExtensions: true,
//   });

//   form.parse(req, async (err, fields, files) => {
//     if (err) {
//       console.error('âŒ Form parse error:', err);
//       return res.status(500).json({ error: 'Form parse error', detail: err.message });
//     }

//     const rawFile = Array.isArray(files.file) ? files.file[0] : files.file;
//     if (!rawFile || !rawFile.filepath) {
//       return res.status(400).json({ error: 'Audio file is missing' });
//     }

//     const openAIKey = process.env.OPENAI_API_KEY;
//     if (!openAIKey) {
//       return res.status(500).json({ error: 'Missing OpenAI API key' });
//     }

//     try {
//       // âœ… Whisper API with axios + form-data
//       const formData = new FormData();
//       formData.append('file', fs.createReadStream(rawFile.filepath));
//       formData.append('model', 'whisper-1');

//       console.log('ðŸ“¤ Sending to Whisper...');
//       const whisperRes = await axios.post(
//         'https://api.openai.com/v1/audio/transcriptions',
//         formData,
//         {
//           headers: {
//             Authorization: `Bearer ${openAIKey}`,
//             ...formData.getHeaders(), // ðŸ‘ˆ åŒ…å« multipart boundary
//           },
//         }
//       );

//       const transcript = whisperRes.data.text;
//       console.log('âœ… Whisper transcript:', transcript);

//       // ðŸ” GPT summary
//       const gptRes = await axios.post(
//         'https://api.openai.com/v1/chat/completions',
//         {
//           model: 'gpt-4o-mini',
//           //messages: [{ role: 'user', content: `please summarize the following and split into explicit and tacit knowledge:\n\n${transcript}` }],
//           messages: [{
//             role: 'user',
//             content: `Please summarize the following meeting transcript and split the content into explicit and tacit knowledge. Only include conversation related to the team's project work or collaboration.\n\n- Explicit knowledge refers to documented, factual information such as data, specifications, or user feedback.\n- Tacit knowledge refers to intuitive insights, experience-based observations, or subjective impressions shared by team members.\n\nSummarize the conversation and categorize the relevant points into the two types.\n\nTranscript:\n\n${transcript}`
//           }]
//         },
//         {
//           headers: {
//             Authorization: `Bearer ${openAIKey}`,
//             'Content-Type': 'application/json',
//           },
//         }
//       );

//       return res.status(200).json({
//         transcript,
//         summary: gptRes.data.choices[0].message.content,
//       });
//     } catch (err) {
//       console.error(' Error during processing:', err?.response?.data || err.message);
//       return res.status(500).json({
//         error: 'Processing failed',
//         detail: err?.response?.data || err.message,
//       });
//     }
//   });
// }

// api/analyze.js
// éŸ³é¢‘å¤„ç†ä¸­å¿ƒ - æŽ¥æ”¶éŸ³é¢‘å—ï¼Œè½¬å½•å¹¶åˆ†æž
// api/analyze.js - Fixed version with proper ES module imports
import { IncomingForm } from 'formidable';
import fs from 'fs';
import axios from 'axios';
import FormData from 'form-data';

// Use ES module import for config
import config from '../lib/config.js';  // Note: .js extension is required for ES modules

// Vercel configuration for file upload handling
export const apiConfig = {
  api: {
    bodyParser: false,  // Disable default body parsing for file uploads
  },
};

export default async function handler(req, res) {
  // Add CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
  
  // Handle OPTIONS request for CORS preflight
  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }
  
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  // Check OpenAI configuration
  if (!config.openai.apiKey) {
    console.error('OpenAI API key not configured');
    return res.status(500).json({ error: 'Server configuration error: Missing OpenAI API key' });
  }

  // Create form parser for file uploads
  const form = new IncomingForm({
    uploadDir: '/tmp',        // Vercel's temp directory
    keepExtensions: true,     // Keep file extensions
    maxFileSize: 50 * 1024 * 1024,  // 50MB limit
  });

  // Parse uploaded form data
  form.parse(req, async (err, fields, files) => {
    if (err) {
      console.error('âŒ Form parse error:', err);
      return res.status(500).json({ 
        error: 'Form parse error', 
        detail: config.isDevelopment ? err.message : undefined 
      });
    }

    // Get uploaded audio file
    const rawFile = Array.isArray(files.file) ? files.file[0] : files.file;
    if (!rawFile || !rawFile.filepath) {
      return res.status(400).json({ error: 'Audio file is missing' });
    }

    console.log('ðŸ“ Received audio file:', {
      name: rawFile.originalFilename,
      size: rawFile.size,
      type: rawFile.mimetype
    });

    try {
      // ========== Step 1: Speech to Text (Whisper) ==========
      const formData = new FormData();
      formData.append('file', fs.createReadStream(rawFile.filepath));
      formData.append('model', config.openai.whisperModel || 'whisper-1');
      
      // Optional: Add language hint
      // formData.append('language', 'en'); // or 'zh' for Chinese

      console.log('ðŸ”¤ Sending to Whisper API...');
      
      const whisperRes = await axios.post(
        `${config.openai.apiUrl}/audio/transcriptions`,
        formData,
        {
          headers: {
            Authorization: `Bearer ${config.openai.apiKey}`,
            ...formData.getHeaders(), // Include multipart boundary
          },
          timeout: 60000, // 60 seconds timeout
        }
      );

      const transcript = whisperRes.data.text || '';
      console.log('âœ… Whisper transcript received, length:', transcript.length);

      // Handle empty transcripts
      if (!transcript || transcript.trim() === '') {
        console.warn('âš ï¸ Empty transcript from Whisper');
        return res.status(200).json({
          success: true,
          transcript: '[No speech detected in audio]',
          summary: 'No content to analyze',
          metadata: {
            audioSize: rawFile.size,
            transcriptLength: 0,
            processedAt: new Date().toISOString(),
          }
        });
      }

      // ========== Step 2: Text Analysis (GPT) ==========
      
      // Build analysis prompt
      const systemPrompt = `You are a meeting assistant analyzing team discussions. 
      Focus on project-related content and collaboration.
      Categorize insights into explicit knowledge (documented facts) and tacit knowledge (experiential insights).`;
      
      const userPrompt = `Please summarize the following meeting transcript and split the content into explicit and tacit knowledge.

Only include conversation related to the team's project work or collaboration.

- Explicit knowledge refers to documented, factual information such as data, specifications, or user feedback.
- Tacit knowledge refers to intuitive insights, experience-based observations, or subjective impressions shared by team members.

Summarize the conversation and categorize the relevant points into the two types.

Transcript:

${transcript}`;

      console.log('ðŸ¤– Sending to GPT for analysis...');
      
      const gptRes = await axios.post(
        `${config.openai.apiUrl}/chat/completions`,
        {
          model: config.openai.model || 'gpt-4o-mini', // Use configured model
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
          ],
          temperature: 0.3,  // Lower temperature for more consistent output
          max_tokens: 500,   // Limit response length
        },
        {
          headers: {
            Authorization: `Bearer ${config.openai.apiKey}`,
            'Content-Type': 'application/json',
          },
          timeout: 30000, // 30 seconds timeout
        }
      );

      // Check GPT response
      if (!gptRes.data?.choices?.[0]?.message?.content) {
        console.warn('âš ï¸ GPT returned empty response');
        return res.status(200).json({
          success: true,
          transcript,
          summary: 'Unable to generate summary',
          metadata: {
            audioSize: rawFile.size,
            transcriptLength: transcript.length,
            processedAt: new Date().toISOString(),
          }
        });
      }

      const summary = gptRes.data.choices[0].message.content;
      console.log('âœ… GPT analysis complete');

      // ========== Step 3: Clean up temporary file ==========
      try {
        fs.unlinkSync(rawFile.filepath);
        console.log('ðŸ—‘ï¸ Temporary file cleaned up');
      } catch (cleanupErr) {
        console.warn('Failed to clean up temp file:', cleanupErr.message);
      }

      // ========== Step 4: Return results ==========
      return res.status(200).json({
        success: true,
        transcript,
        summary,
        metadata: {
          audioSize: rawFile.size,
          transcriptLength: transcript.length,
          processedAt: new Date().toISOString(),
        }
      });
      
    } catch (err) {
      console.error('âŒ Error during processing:', err?.response?.data || err.message);
      
      // Clean up temp file on error
      try {
        fs.unlinkSync(rawFile.filepath);
      } catch (cleanupErr) {
        // Ignore cleanup errors
      }
      
      // Return different responses based on error type
      if (err.response?.status === 401) {
        return res.status(500).json({
          error: 'Authentication failed',
          detail: 'Invalid or missing API key'
        });
      }
      
      if (err.response?.status === 429) {
        return res.status(429).json({
          error: 'Rate limit exceeded',
          detail: 'Too many requests, please try again later'
        });
      }
      
      // Return detailed error in development
      if (config.isDevelopment) {
        return res.status(500).json({
          error: 'Processing failed',
          detail: err?.response?.data || err.message,
          stack: err.stack
        });
      }
      
      // Return generic error in production
      return res.status(500).json({
        error: 'Processing failed',
        detail: 'An error occurred while processing the audio'
      });
    }
  });
}

// ========== æ³¨é‡ŠæŽ‰çš„ AssemblyAI ä»£ç è¯´æ˜Ž ==========
/*
æ‚¨çš„ä»£ç ä¸­æœ‰æ³¨é‡ŠæŽ‰çš„ AssemblyAI å®žçŽ°ï¼Œè¿™æ˜¯å¦ä¸€ç§éŸ³é¢‘å¤„ç†æ–¹æ¡ˆï¼š

AssemblyAI ä¼˜åŠ¿ï¼š
1. æ”¯æŒè¯´è¯äººåˆ†ç¦»ï¼ˆspeaker diarizationï¼‰- å¯ä»¥è¯†åˆ«ä¸åŒçš„è¯´è¯äºº
2. æ›´å‡†ç¡®çš„é•¿éŸ³é¢‘è½¬å½•
3. æ”¯æŒå¤šç§è¯­è¨€è‡ªåŠ¨æ£€æµ‹

å¦‚æžœå°†æ¥éœ€è¦è¯´è¯äººåˆ†ç¦»åŠŸèƒ½ï¼Œå¯ä»¥ï¼š
1. åœ¨ Vercel æ·»åŠ  ASSEMBLYAI_API_KEY
2. åœ¨ config.js æ·»åŠ  AssemblyAI é…ç½®
3. å¯ç”¨æ³¨é‡Šçš„ä»£ç 

å½“å‰ä½¿ç”¨ OpenAI Whisper çš„åŽŸå› ï¼š
1. ç»Ÿä¸€ä½¿ç”¨ OpenAI æœåŠ¡ï¼Œå‡å°‘ä¾èµ–
2. Whisper å¯¹çŸ­éŸ³é¢‘ï¼ˆ30ç§’ç‰‡æ®µï¼‰æ•ˆæžœå¾ˆå¥½
3. æˆæœ¬æ›´ä½Žï¼Œé›†æˆæ›´ç®€å•
*/







// // /api/analyze.js  â€”â€” AssemblyAI (speaker diarization), no GPT
// import { IncomingForm } from 'formidable';
// import fs from 'fs';
// import axios from 'axios';

// export const config = { api: { bodyParser: false } };

// // AssemblyAI endpoints
// const AAI_KEY = process.env.ASSEMBLYAI_API_KEY;
// const AAI_UPLOAD = 'https://api.assemblyai.com/v2/upload';
// const AAI_TRANSCRIPT = 'https://api.assemblyai.com/v2/transcript';

// export default async function handler(req, res) {
//   if (req.method !== 'POST') return res.status(405).json({ error: 'Method not allowed' });
//   if (!AAI_KEY) return res.status(500).json({ error: 'Missing ASSEMBLYAI_API_KEY' });

//   const form = new IncomingForm({ uploadDir: '/tmp', keepExtensions: true });

//   form.parse(req, async (err, fields, files) => {
//     if (err) {
//       console.error('âŒ Form parse error:', err);
//       return res.status(500).json({ error: 'Form parse error', detail: err.message });
//     }

//     const rawFile = Array.isArray(files.file) ? files.file[0] : files.file;
//     if (!rawFile?.filepath) return res.status(400).json({ error: 'Audio file is missing' });

//     try {
//       // 1) Upload audio bytes to AssemblyAI
//       const uploadRes = await axios.post(
//         AAI_UPLOAD,
//         fs.createReadStream(rawFile.filepath),
//         {
//           headers: { authorization: AAI_KEY, 'content-type': 'application/octet-stream' },
//           maxContentLength: Infinity,
//           maxBodyLength: Infinity,
//           timeout: 60_000
//         }
//       );

//       // 2) Create transcript with speaker labels
//       const createRes = await axios.post(
//         AAI_TRANSCRIPT,
//         {
//           audio_url: uploadRes.data.upload_url,
//           speaker_labels: true,      // ðŸ‘ˆ å…³é”®ï¼šå¼€å¯è¯´è¯äººåˆ†ç¦»
//           punctuate: true,
//           format_text: true,
//           // language_code: 'en_us'   // å¯é€‰
//         },
//         { headers: { authorization: AAI_KEY }, timeout: 30_000 }
//       );

//       const jobId = createRes.data.id;

//       // 3) Poll until completed
//       const tr = await pollAAI(jobId);

//       // 4) Build utterances + labeled transcript
//       const utterances = normalizeUtterances(tr?.utterances || []);
//       const transcript = utterances.length
//         ? utterances.map(u => `${u.speaker_label}: ${u.text}`).join('\n')
//         : (tr?.text || '');

//       return res.status(200).json({
//         provider: 'assemblyai',
//         duration_sec: tr?.audio_duration || null,
//         transcript,
//         utterances
//       });

//     } catch (e) {
//       const detail = e?.response?.data || e?.message || String(e);
//       console.error('ðŸ”¥ analyze error:', detail);
//       return res.status(500).json({ error: 'Processing failed', detail });
//     } finally {
//       try { fs.unlinkSync(rawFile.filepath); } catch {}
//     }
//   });
// }

// // ---- Helpers ----
// async function pollAAI(id) {
//   const headers = { authorization: AAI_KEY };
//   const maxAttempts = 100; // ~150s
//   for (let i = 0; i < maxAttempts; i++) {
//     await sleep(1500);
//     try {
//       const r = await axios.get(`${AAI_TRANSCRIPT}/${id}`, { headers, timeout: 25_000 });
//       const data = r.data || {};
//       if (data.status === 'completed') return data;
//       if (data.status === 'error') throw new Error(data.error || 'AAI error');
//       // queued / processing -> continue
//     } catch (e) {
//       if (i > 5) console.warn(`Polling #${i} error`, e?.message || e);
//     }
//   }
//   throw new Error('Timeout polling transcript');
// }

// function normalizeUtterances(raw = []) {
//   // AAI çš„ speaker å¸¸è§ä¸º 'A','B','C'...ï¼›ä¹Ÿå¯èƒ½æ˜¯æ•°å­—ï¼Œç»Ÿä¸€æˆ SPEAKER_X
//   return raw.map(u => ({
//     speaker_label: `SPEAKER_${u.speaker ?? 'X'}`,
//     start_ms: Math.round((u.start || 0) * 1000),
//     end_ms: Math.round((u.end || 0) * 1000),
//     text: u.text || ''
//   }));
// }

// function sleep(ms) { return new Promise(r => setTimeout(r, ms)); }
